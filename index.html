<!DOCTYPE html>
<html lang="en-US">
  <head>
    <title>Yang Liu | Microsoft</title>

    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Yang Liu is a Senior Researcher at Microsoft working on Natural Language Processing.">
    
    <meta name="keywords" content="text summarization, yang liu, natrual language processing, text generation">
    
    

    <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üêè</text></svg>">


    <link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin=anonymous>
    <link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin=anonymous>

    <link rel="stylesheet" href="./assets/css/style.css">
  </head>
  <body>
    <div class="wrapper">
      <header>
        
        
        <a class="image avatar"><img src="./assets/img/avatar.jpg" alt="avatar" onContextMenu="return false" /></a>
        

        <h1>Yang Liu <span style="font-family: NotoSerifSC; font-weight: 400">(ÂàòÊ¥ã)</span ></h1> 
        
        Senior Researcher, Microsoft
        
        <br>
        <email>yaliu10 (at) microsoft.com</email>
        <br>
        <br>
        
<div class="social-icons">
  
          <a style="margin: 0 5px 0 0" href="https://twitter.com/nlpyang">
            <i class="fab fa-twitter" style="font-size:1.2rem"></i>
          </a>         

          <a style="margin: 0 5px 0 0" href="https://scholar.google.com/citations?user=HxTr-CtMdrsC">
            <i class="ai ai-google-scholar" style="font-size:1.2rem"></i>
          </a>         
          
          
              <a style="margin: 0 5px 0 0" href="https://www.linkedin.com/in/yang-liu-2a0881b1/">
            <i class="fab fa-linkedin"></i>
          </a>
            
        </div>
        <br>
      </header>
      <section>

      <h3 id="about-me">About Me</h3>
<p>I am a Senior Researcher at <a href="https://www.microsoft.com/en-us/research/group/knowledge-and-language/#!people/">Microsoft</a> working on Natural Language Processing.
I received my PhD degree from  <a href="http://web.inf.ed.ac.uk/ilcc">University of Edinburgh</a>  in 2020, under supvervision of Prof. Mirella Lapata. 
My main research interest is Text Summarization, Text Generation and Structure Learning.</p>

<h3 id="news">News</h3>
<ul>
  <li><strong>[2024.07]</strong>   üö©We present <a href="https://arxiv.org/pdf/2406.07522" target="_blank"><b>Samba</b></a>, a powerful hybrid LLM.</li>
  <li><strong>[2024.05]</strong>   üö©I build <a href="https://openai.com/index/introducing-openai-japan/"<b>GPT-4 Japanese</b></a>.</li>
  <li><strong>[2023.04]</strong>   I will serve as an Area Chair for <b>NeurIPS 2024</b>.</li>
  <li><strong>[2023.03]</strong>   We proposed <a href="https://arxiv.org/abs/2303.16634"><b>G-Eval</b></a>: <b>NLG Evaluation using GPT-4 with Better Human Alignment</b>. It uses GPT-4 as an NLG evaluator. And the most interesting thing we find is GPT-based evaluator has a preference bias towards GPT outputs, even over human high-quality outputs.</li>
  <li><strong>[2022.11]</strong>   We release <a href="https://arxiv.org/abs/2211.09783" <b>UniSumm</b></a>, a SOTA few-shot summarization model, along with a new benchmark <b>SummZoo</b>.</li>
  <li><strong>[2022.10]</strong>   Five papers accepted by <b>EMNLP 2022</b> and its findings.</li>
  <li><strong>[2022.04]</strong>   I will be Senior Area Chair for <b>AACL 2022</b> and Area Chair for <b>COLING 2022</b>.</li>
  <li><strong>[2022.03]</strong>   Two papers accepted by ACL 2022 and its findings.</li>
  <li><strong>[2022.01]</strong>   Give a talk on <b>Dialogue Summarization</b> at Amazon. <a href="./Dialogue_Summarization.pdf"> (Slides)</a></li>
  <li><strong>[2021.03]</strong>   Two Long papers and one short paper accepted by NAACL 2021.</li>
  <li><strong>[2021.01]</strong>   Our RE-T5 model achieved the first place in <a href="https://inklab.usc.edu/CommonGen/"> CommonGen</a> competition.</li>
  <li><strong>[2020.10]</strong>   Achieved the first place in <a href="https://competitions.codalab.org/competitions/18814#results">FEVER</a> competition.</li>
</ul>

<h3 id="publications">Selected Publications</h3>
   <li>
    <p><a href="https://arxiv.org/pdf/2406.07522" target="_blank">Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling</a>
<br />
Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, Weizhu Chen
<br /></p>
  </li>

<ul>
   <li>
    <p><a href="https://arxiv.org/abs/2306.11197" target="_blank">Sparse Modular Activation for Efficient Sequence Modeling</a>
<br />
Liliang Ren, Yang Liu, Shuohang Wang, Yichong Xu, Chenguang Zhu, ChengXiang Zhai <i>Neurips 2023</i> 
<br /></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2303.16634" target="_blank">G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment</a>
<br />
Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, Chenguang Zhu <i>EMNLP 2023</i> 
<br /></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2210.06277" target="_blank">Task Compass: Scaling Multi-task Pre-training with Task Prefix</a>
<br />
Zhuosheng Zhang, Shuohang Wang, Yichong Xu, Yuwei Fang, Wenhao Yu, Yang Liu, Hai Zhao, Chenguang Zhu, Michael Zeng. <i>EMNLP 2022</i> findings
<br /></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2210.07197" target="_blank">Towards a Unified Multi-Dimensional Evaluator for Text Generation</a>
<br />
Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Pengfei Liu, Chenguang Zhu, Heng Ji, Jiawei Han. <i>EMNLP 2022</i>
<br /></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2202.04824" target="_blank">AdaPrompt: Adaptive Model Training for Prompt-based NLP</a>
<br />
Yulong Chen, Yang Liu, Li Dong, Shuohang Wang, Chenguang Zhu, Michael Zeng and Yue Zhang. <i>EMNLP 2022</i> findings.
<br /></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2201.12502" target="_blank">Unsupervised Summarization with Customized Granularities</a>
<br />
Ming Zhong, Yang Liu, Suyu Ge, Yuning Mao, Yizhu Jiao, Xingxing Zhang, Yichong Xu, Chenguang Zhu, Michael Zeng, Jiawei Han. <em>EMNLP 2022</em> findings.
<br /></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2203.08773" target="_blank">Training Data is More Valuable than You Think: A Simple and Effective Method by Retrieving from Training Data </a>
<br />
Shuohang Wang, Yichong Xu, Yuwei Fang, Yang Liu, Siqi Sun, Ruochen Xu, Chenguang Zhu and Michael Zeng. <em>ACL 2022</em>.
<br /></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2109.02492" target="_blank">DialogLM: Pre-trained Model for Long Dialogue Understanding and Summarization</a>
<br />
Ming Zhong, Yang Liu, Yichong Xu, Chenguang Zhu, Michael Zeng. <em>AAAI 2022</em>.
<br /></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2108.13487" target="_blank">Want To Reduce Labeling Cost? GPT-3 Can Help</a>
<br />
Shuohang Wang, Yang Liu, Yichong Xu, Chenguang Zhu, Michael Zeng. <em>EMNLP 2021</em> findings.
<br /></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2105.11174" target="_blank">Retrieval Enhanced Model for Commonsense Generation</a>
<br />
Han Wang, Yang Liu, Chenguang Zhu, Linjun Shou, Ming Gong, Yichong Xu, Michael Zeng. <em>ACL 2021</em> findings. <a href="https://github.com/HanNight/RE-T5" target="_blank">[code]</a>
<br /></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2105.06762" target="_blank">DialogSum: A Real-Life Scenario Dialogue Summarization Dataset</a>
<br />
Yulong Chen , Yang Liu , Liang Chen, Yue Zhang. <em>ACL 2021</em> findings. <a href="https://github.com/cylnlp/DialSumm/" target="_blank">[dataset]</a>
<br /></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2103.06410" target="_blank">MediaSum: A Large-scale Media Interview Dataset for Dialogue Summarization</a>
<br />
Chenguang Zhu<em>, Yang Liu</em>, Jie Mei, Michael Zeng.  <em>NAACL 2021</em>. (* indicates equal contribution)  <a href="https://github.com/zcgzcgzcg1/MediaSum/" target="_blank">[dataset]</a>
<br /></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2009.07032" target="_blank">Noisy Self-Knowledge Distillation for Text Summarization</a>
<br />
Yang Liu, Sheng Shen and Mirella Lapata.  <em>NAACL 2021</em>.
<br /></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/abs/2104.05938" target="_blank">QMSum: A New Benchmark for Query-based Multi-domain Meeting Summarization</a>
<br />
Ming Zhong, Da Yin, Tao Yu, Ahmad Zaidi, Mutethia Mutuma, Rahul Jha, Ahmed Hassan Awadallah, Asli Celikyilmaz, Yang Liu, Xipeng Qiu, Dragomir Radev.  <em>NAACL 2021</em>.
<br /></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/pdf/1908.08345" target="_blank">Text Summarization with Pretrained Encoders</a>
<br />
Yang Liu and Mirella Lapata. <em>EMNLP 2019</em>. <a href="https://github.com/nlpyang/PreSumm" target="_blank">[code]</a> <a href="./emnlp19_supp.pdf">[supplementary material]</a>
<br /></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/pdf/1905.13164.pdf" target="_blank">Hierarchical Transformers for Multi-document Summarization</a>
<br />
Yang Liu and Mirella Lapata. <em>ACL 2019</em>.   <a href="https://github.com/nlpyang/hiersumm" target="_blank">[code]</a>
<br /></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/pdf/1906.04687" target="_blank">Generating Summaries with Topic Templates and Structured Convolutional Decoders</a>
<br />
Laura Perez-Beltrachini, Yang Liu, Mirella Lapata. <em>ACL 2019</em>.
<br /></p>
  </li>
  <li>
    <p><a href="https://arxiv.org/pdf/1903.10318.pdf" target="_blank">Fine-tune BERT for Extractive Summarization</a>
<br />
Yang Liu. <a href="https://github.com/nlpyang/BertSum" target="_blank">[code]</a>
<br /></p>
  </li>
  <li>
    <p><a href="https://www.aclweb.org/anthology/N19-1173" target="_blank">Single Document Summarization as Tree Induction</a>
<br />
Yang Liu, Ivan Titov and Mirella Lapata. <em>NAACL 2019</em>. <a href="https://github.com/nlpyang/SUMO" target="_blank">[code]</a>
<br /></p>
  </li>

</ul>

<h3 id="services">Services</h3>

<ul>
  <li>Action Editor:
ACL Rolling Review</li>
  <li>Senior Area Chair:
AACL 2022</li>
  <li>Area Chair:
NAACL 2021, COLING 2022</li>
  <li>Conference Program Committee Member:
ACL 2016-2021, EMNLP 2017-2021, NAACL 2017-2020</li>
</ul>

<h3 id="education">Education</h3>

<ul>
  <li>PhD, Institute for Language, Cognition and Computation, University of Edinburgh
<br />
Supervisor: Prof. Mirella Lapata</li>
  <li>MSc, Institute of Computational Linguistics, Peking University
<br />
Supervisor: Prof. Sujian Li</li>
  <li>BE, School of Computer Science, Tianjin University</li>
</ul>


      <br>


      </section>
      <footer>
        
        <br>
        <br>
        <br>
      </footer>
    </div>
    <script src="/assets/js/scale.fix.js"></script>
    <script>var clicky_site_ids = clicky_site_ids || []; clicky_site_ids.push(101341821);</script>
    <script async src="//static.getclicky.com/js"></script>

  </body>
</html>
